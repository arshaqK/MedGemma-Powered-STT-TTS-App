{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install fastapi uvicorn openai faster_whisper  pyngrok fpdf torch torchaudio transformers soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-22T10:03:27.769386Z",
     "iopub.status.busy": "2025-07-22T10:03:27.769089Z",
     "iopub.status.idle": "2025-07-22T10:03:42.673945Z",
     "shell.execute_reply": "2025-07-22T10:03:42.661863Z",
     "shell.execute_reply.started": "2025-07-22T10:03:27.769356Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyngrok'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfaster_whisper\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WhisperModel               \u001b[38;5;66;03m# ðŸ†• lowâ€‘latency decoder\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, VitsModel\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyngrok\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ngrok\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfpdf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FPDF\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msoundfile\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msf\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pyngrok'"
     ]
    }
   ],
   "source": [
    "\n",
    "from fastapi import FastAPI, Form, UploadFile, File, WebSocket, WebSocketDisconnect\n",
    "from fastapi.responses import HTMLResponse, FileResponse, JSONResponse\n",
    "from openai import OpenAI\n",
    "from faster_whisper import WhisperModel               # ðŸ†• lowâ€‘latency decoder\n",
    "from transformers import AutoTokenizer, VitsModel\n",
    "from pyngrok import ngrok\n",
    "from fpdf import FPDF\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import torch, os, time, uvicorn, asyncio, tempfile, threading\n",
    "\n",
    "# === MedGemma via Ollama ===\n",
    "OLLAMA_BASE_URL = \"http://203.124.40.57:11434/v1\"\n",
    "VISION_MODEL = \"puyangwang/medgemma-27b-it:q8\"\n",
    "client = OpenAI(base_url=OLLAMA_BASE_URL, api_key=\"ollama\")\n",
    "\n",
    "# === STT: fasterâ€‘whisper (Largeâ€‘v3) ===\n",
    "STT_MODEL_SIZE = \"large-v3\"   # or \"medium\" for <Â 8â€¯GB GPUs\n",
    "stt_model = WhisperModel(STT_MODEL_SIZE, device=\"cuda\", compute_type=\"float16\")\n",
    "# (fasterâ€‘whisper automatically handles batching + GPU kernels)\n",
    "\n",
    "# === TTS: Urdu VITS ===\n",
    "tts_model_id = \"hamzamunir/mms-tts-urdu-vits-finetuned-aliakbar\"\n",
    "tts_tokenizer = AutoTokenizer.from_pretrained(tts_model_id)\n",
    "tts_model     = VitsModel.from_pretrained(tts_model_id).to(\"cuda\")\n",
    "\n",
    "# === Ngrok ===\n",
    "NGROK_AUTH_TOKEN = \"YOUR_NGROK_TOKEN_HERE\"\n",
    "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
    "\n",
    "# Kill all existing tunnels\n",
    "ngrok.kill()\n",
    "\n",
    "# Start a fresh tunnel\n",
    "public_url = ngrok.connect(8000)\n",
    "print(f\"ðŸŒ Public URL: {public_url}\")\n",
    "\n",
    "app = FastAPI(title=\"Medical AI Diagnosis â€“ Lowâ€‘Latency\")\n",
    "\n",
    "# === Load UI ===\n",
    "with open(\"/kaggle/input/oooooooook/index.html\", \"r\", encoding=\"utf-8\") as f:\n",
    "    INDEX_HTML = f.read()\n",
    "\n",
    "# === System prompts for MedGemma ===\n",
    "system_prompts = [\n",
    "    \"You are a friendly, Urdu/English-speaking AI medical assistant trained to perform initial patient intake.\",\n",
    "    \"You will greet the patient, ask standard intake questions (e.g., name, age, symptoms, allergies, medications, history), and confirm responses.\",\n",
    "    \"Once enough data is collected, summarize the encounter in FHIR format using a QuestionnaireResponse or a Bundle with Patient, Condition, and Observation resources.\",\n",
    "    \"Always include: Full name, Gender, Age or birthdate, Reason for visit, Symptoms with duration, Allergies and medications (if mentioned).\",\n",
    "    \"Output the FHIR JSON at the end, with one resource per section.\",\n",
    "    \"Respond in a polite, conversational tone.\"\n",
    "]\n",
    "\n",
    "@app.get(\"/\", response_class=HTMLResponse)\n",
    "async def serve_index():\n",
    "    return INDEX_HTML\n",
    "\n",
    "@app.post(\"/\", response_class=HTMLResponse)\n",
    "async def medgemma_chat(prompt: str = Form(...)):\n",
    "    msgs = [{\"role\": \"system\", \"content\": p} for p in system_prompts] + [{\"role\": \"user\", \"content\": prompt}]\n",
    "    start = time.time()\n",
    "    res = client.chat.completions.create(model=VISION_MODEL, messages=msgs, max_tokens=1024, temperature=0.2)\n",
    "    duration = round(time.time() - start, 2)\n",
    "    answer = res.choices[0].message.content.strip()\n",
    "    return HTMLResponse(INDEX_HTML.replace(\"{{ prompt or '' }}\", prompt)\n",
    "                                   .replace(\"{{ response }}\", answer)\n",
    "                                   .replace(\"{{ processing_time }}\", str(duration)))\n",
    "\n",
    "# === traditional /stt, /tts, /generate_pdf endpoints remain unchanged (kept for fallback) ===\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# ðŸ–¥ï¸  REALâ€‘TIME STT OVER WEBSOCKET â€“ ultraâ€‘low latency\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "BUFFER_DURATION  = 1.0        # seconds per decode window\n",
    "SAMPLE_RATE      = 16000      # Whisper expects 16â€¯kHz\n",
    "BUFFER_SAMPLES   = int(BUFFER_DURATION * SAMPLE_RATE)\n",
    "\n",
    "@app.websocket(\"/ws/audio\")\n",
    "async def ws_audio(websocket: WebSocket):\n",
    "    await websocket.accept()\n",
    "    ring_buffer = np.zeros(0, dtype=np.float32)\n",
    "\n",
    "    async def decode_and_respond(audio_np: np.ndarray):\n",
    "        \"\"\"Background decode to avoid blocking the ws receive loop.\"\"\"\n",
    "        segments, _ = stt_model.transcribe(audio_np, language=\"ur\", vad_filter=False, beam_size=1)\n",
    "        transcript = \"\".join([seg.text for seg in segments]).strip()\n",
    "        if transcript:\n",
    "            await websocket.send_text(transcript)\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            packet = await websocket.receive_bytes()            # raw PCM int16\n",
    "            chunk = np.frombuffer(packet, dtype=np.int16).astype(np.float32) / 32768.0\n",
    "            ring_buffer = np.concatenate((ring_buffer, chunk))\n",
    "\n",
    "            if ring_buffer.shape[0] >= BUFFER_SAMPLES:\n",
    "                # Take first BUFFER_SAMPLES for decoding; keep remainder\n",
    "                to_decode, ring_buffer = ring_buffer[:BUFFER_SAMPLES], ring_buffer[BUFFER_SAMPLES:]\n",
    "                # Launch background task so networking thread keeps up\n",
    "                asyncio.create_task(decode_and_respond(to_decode))\n",
    "    except WebSocketDisconnect:\n",
    "        pass\n",
    "\n",
    "# === Run ===\n",
    "\n",
    "def _run():\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
    "threading.Thread(target=_run, daemon=True).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7918007,
     "sourceId": 12541805,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
